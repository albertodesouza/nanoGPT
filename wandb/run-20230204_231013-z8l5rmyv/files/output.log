/home/lume/.local/lib/python3.8/site-packages/torch/nn/utils/stateless.py:44: UserWarning: functional_call was passed multiple values for tied weights. This behavior is deprecated and will be an error in future versions
  warnings.warn("functional_call was passed multiple values for tied weights. "
/home/lume/.local/lib/python3.8/site-packages/torch/nn/utils/stateless.py:44: UserWarning: functional_call was passed multiple values for tied weights. This behavior is deprecated and will be an error in future versions
  warnings.warn("functional_call was passed multiple values for tied weights. "
concurrent.futures.process._RemoteTraceback:
"""
Traceback (most recent call last):
  File "/usr/lib/python3.8/concurrent/futures/process.py", line 239, in _process_worker
    r = call_item.fn(*call_item.args, **call_item.kwargs)
  File "/home/lume/.local/lib/python3.8/site-packages/torch/_inductor/codecache.py", line 497, in _worker_compile
    kernel.precompile(warm_cache_only_with_cc=cc)
  File "/home/lume/.local/lib/python3.8/site-packages/torch/_inductor/triton_ops/autotune.py", line 59, in precompile
    self.launchers = [
  File "/home/lume/.local/lib/python3.8/site-packages/torch/_inductor/triton_ops/autotune.py", line 60, in <listcomp>
    self._precompile_config(c, warm_cache_only_with_cc)
  File "/home/lume/.local/lib/python3.8/site-packages/torch/_inductor/triton_ops/autotune.py", line 73, in _precompile_config
    triton.compile(
  File "/home/lume/.local/lib/python3.8/site-packages/triton/compiler.py", line 1239, in compile
    so = _build(fn.__name__, src_path, tmpdir)
  File "/home/lume/.local/lib/python3.8/site-packages/triton/compiler.py", line 1169, in _build
    ret = subprocess.check_call(cc_cmd)
  File "/usr/lib/python3.8/subprocess.py", line 364, in check_call
    raise CalledProcessError(retcode, cmd)
subprocess.CalledProcessError: Command '['/usr/bin/gcc', '/tmp/tmpq6ypw6hj/main.c', '-O3', '-I/usr/local/cuda/include', '-I/usr/include/python3.8', '-I/tmp/tmpq6ypw6hj', '-shared', '-fPIC', '-lcuda', '-o', '/tmp/tmpq6ypw6hj/triton_.cpython-38-x86_64-linux-gnu.so']' returned non-zero exit status 1.
"""
The above exception was the direct cause of the following exception:
Traceback (most recent call last):
  File "/home/lume/.local/lib/python3.8/site-packages/torch/_dynamo/output_graph.py", line 680, in call_user_compiler
    compiled_fn = compiler_fn(gm, self.fake_example_inputs())
  File "/home/lume/.local/lib/python3.8/site-packages/torch/_dynamo/debug_utils.py", line 1032, in debug_wrapper
    compiled_gm = compiler_fn(gm, example_inputs, **kwargs)
  File "/home/lume/.local/lib/python3.8/site-packages/torch/__init__.py", line 1234, in __call__
    return self.compile_fn(model_, inputs_)
  File "/home/lume/.local/lib/python3.8/site-packages/torch/_inductor/compile_fx.py", line 398, in compile_fx
    return aot_autograd(
  File "/home/lume/.local/lib/python3.8/site-packages/torch/_dynamo/optimizations/training.py", line 78, in compiler_fn
    cg = aot_module_simplified(gm, example_inputs, **kwargs)
  File "/home/lume/.local/lib/python3.8/site-packages/torch/_functorch/aot_autograd.py", line 2355, in aot_module_simplified
    compiled_fn = create_aot_dispatcher_function(
  File "/home/lume/.local/lib/python3.8/site-packages/torch/_dynamo/utils.py", line 88, in time_wrapper
    r = func(*args, **kwargs)
  File "/home/lume/.local/lib/python3.8/site-packages/torch/_functorch/aot_autograd.py", line 2052, in create_aot_dispatcher_function
    compiled_fn = compiler_fn(flat_fn, fake_flat_tensor_args, aot_config)
  File "/home/lume/.local/lib/python3.8/site-packages/torch/_functorch/aot_autograd.py", line 1307, in aot_wrapper_dedupe
    return compiler_fn(flat_fn, leaf_flat_args, aot_config)
  File "/home/lume/.local/lib/python3.8/site-packages/torch/_functorch/aot_autograd.py", line 957, in aot_dispatch_base
    compiled_fw = aot_config.fw_compiler(fw_module, flat_args)
  File "/home/lume/.local/lib/python3.8/site-packages/torch/_dynamo/utils.py", line 88, in time_wrapper
    r = func(*args, **kwargs)
  File "/home/lume/.local/lib/python3.8/site-packages/torch/_inductor/compile_fx.py", line 373, in fw_compiler
    return inner_compile(
  File "/home/lume/.local/lib/python3.8/site-packages/torch/_dynamo/debug_utils.py", line 588, in debug_wrapper
    compiled_fn = compiler_fn(gm, example_inputs, **kwargs)
  File "/home/lume/.local/lib/python3.8/site-packages/torch/_inductor/debug.py", line 223, in inner
    return fn(*args, **kwargs)
  File "/usr/lib/python3.8/contextlib.py", line 75, in inner
    return func(*args, **kwds)
  File "/home/lume/.local/lib/python3.8/site-packages/torch/_inductor/compile_fx.py", line 140, in compile_fx_inner
    compiled_fn = graph.compile_to_fn()
  File "/home/lume/.local/lib/python3.8/site-packages/torch/_inductor/graph.py", line 538, in compile_to_fn
    return self.compile_to_module().call
  File "/home/lume/.local/lib/python3.8/site-packages/torch/_dynamo/utils.py", line 88, in time_wrapper
    r = func(*args, **kwargs)
  File "/home/lume/.local/lib/python3.8/site-packages/torch/_inductor/graph.py", line 527, in compile_to_module
    mod = PyCodeCache.load(code)
  File "/home/lume/.local/lib/python3.8/site-packages/torch/_inductor/codecache.py", line 468, in load
    exec(code, mod.__dict__, mod.__dict__)
  File "/tmp/torchinductor_lume/gi/cgib6qo3eq2ezv2y7jk7fbgsgl6su24mtz6w5sufxf6tpvhqmcy4.py", line 981, in <module>
    async_compile.wait(globals())
  File "/home/lume/.local/lib/python3.8/site-packages/torch/_inductor/codecache.py", line 663, in wait
    scope[key] = result.result()
  File "/home/lume/.local/lib/python3.8/site-packages/torch/_inductor/codecache.py", line 521, in result
    self.future.result()
  File "/usr/lib/python3.8/concurrent/futures/_base.py", line 444, in result
    return self.__get_result()
  File "/usr/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
subprocess.CalledProcessError: Command '['/usr/bin/gcc', '/tmp/tmpq6ypw6hj/main.c', '-O3', '-I/usr/local/cuda/include', '-I/usr/include/python3.8', '-I/tmp/tmpq6ypw6hj', '-shared', '-fPIC', '-lcuda', '-o', '/tmp/tmpq6ypw6hj/triton_.cpython-38-x86_64-linux-gnu.so']' returned non-zero exit status 1.
The above exception was the direct cause of the following exception:
Traceback (most recent call last):
  File "train.py", line 245, in <module>
    losses = estimate_loss()
  File "/home/lume/.local/lib/python3.8/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
  File "train.py", line 205, in estimate_loss
    logits, loss = model(X, Y)
  File "/home/lume/.local/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1482, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/lume/.local/lib/python3.8/site-packages/torch/_dynamo/eval_frame.py", line 83, in forward
    return self.dynamo_ctx(self._orig_mod.forward)(*args, **kwargs)
  File "/home/lume/.local/lib/python3.8/site-packages/torch/_dynamo/eval_frame.py", line 212, in _fn
    return fn(*args, **kwargs)
  File "/home/lume/.local/lib/python3.8/site-packages/torch/_dynamo/eval_frame.py", line 333, in catch_errors
    return callback(frame, cache_size, hooks)
  File "/home/lume/.local/lib/python3.8/site-packages/torch/_dynamo/convert_frame.py", line 480, in _convert_frame
    result = inner_convert(frame, cache_size, hooks)
  File "/home/lume/.local/lib/python3.8/site-packages/torch/_dynamo/convert_frame.py", line 103, in _fn
    return fn(*args, **kwargs)
  File "/home/lume/.local/lib/python3.8/site-packages/torch/_dynamo/utils.py", line 88, in time_wrapper
    r = func(*args, **kwargs)
  File "/home/lume/.local/lib/python3.8/site-packages/torch/_dynamo/convert_frame.py", line 339, in _convert_frame_assert
    return _compile(
  File "/home/lume/.local/lib/python3.8/site-packages/torch/_dynamo/convert_frame.py", line 400, in _compile
    out_code = transform_code_object(code, transform)
  File "/home/lume/.local/lib/python3.8/site-packages/torch/_dynamo/bytecode_transformation.py", line 341, in transform_code_object
    transformations(instructions, code_options)
  File "/home/lume/.local/lib/python3.8/site-packages/torch/_dynamo/convert_frame.py", line 387, in transform
    tracer.run()
  File "/home/lume/.local/lib/python3.8/site-packages/torch/_dynamo/symbolic_convert.py", line 1684, in run
    super().run()
  File "/home/lume/.local/lib/python3.8/site-packages/torch/_dynamo/symbolic_convert.py", line 538, in run
    and self.step()
  File "/home/lume/.local/lib/python3.8/site-packages/torch/_dynamo/symbolic_convert.py", line 501, in step
    getattr(self, inst.opname)(inst)
  File "/home/lume/.local/lib/python3.8/site-packages/torch/_dynamo/symbolic_convert.py", line 1750, in RETURN_VALUE
    self.output.compile_subgraph(self)
  File "/home/lume/.local/lib/python3.8/site-packages/torch/_dynamo/output_graph.py", line 557, in compile_subgraph
    self.compile_and_call_fx_graph(tx, pass2.graph_output_vars(), root)
  File "/home/lume/.local/lib/python3.8/site-packages/torch/_dynamo/output_graph.py", line 604, in compile_and_call_fx_graph
    compiled_fn = self.call_user_compiler(gm)
  File "/home/lume/.local/lib/python3.8/site-packages/torch/_dynamo/output_graph.py", line 685, in call_user_compiler
    raise BackendCompilerFailed(self.compiler_fn, e) from e
torch._dynamo.exc.BackendCompilerFailed: debug_wrapper raised CalledProcessError: Command '['/usr/bin/gcc', '/tmp/tmpq6ypw6hj/main.c', '-O3', '-I/usr/local/cuda/include', '-I/usr/include/python3.8', '-I/tmp/tmpq6ypw6hj', '-shared', '-fPIC', '-lcuda', '-o', '/tmp/tmpq6ypw6hj/triton_.cpython-38-x86_64-linux-gnu.so']' returned non-zero exit status 1.
Set torch._dynamo.config.verbose=True for more information
You can suppress this exception and fall back to eager by setting:
    torch._dynamo.config.suppress_errors = True
concurrent.futures.process._RemoteTraceback:
"""
Traceback (most recent call last):
  File "/usr/lib/python3.8/concurrent/futures/process.py", line 239, in _process_worker
    r = call_item.fn(*call_item.args, **call_item.kwargs)
  File "/home/lume/.local/lib/python3.8/site-packages/torch/_inductor/codecache.py", line 497, in _worker_compile
    kernel.precompile(warm_cache_only_with_cc=cc)
  File "/home/lume/.local/lib/python3.8/site-packages/torch/_inductor/triton_ops/autotune.py", line 59, in precompile
    self.launchers = [
  File "/home/lume/.local/lib/python3.8/site-packages/torch/_inductor/triton_ops/autotune.py", line 60, in <listcomp>
    self._precompile_config(c, warm_cache_only_with_cc)
  File "/home/lume/.local/lib/python3.8/site-packages/torch/_inductor/triton_ops/autotune.py", line 73, in _precompile_config
    triton.compile(
  File "/home/lume/.local/lib/python3.8/site-packages/triton/compiler.py", line 1239, in compile
    so = _build(fn.__name__, src_path, tmpdir)
  File "/home/lume/.local/lib/python3.8/site-packages/triton/compiler.py", line 1169, in _build
    ret = subprocess.check_call(cc_cmd)
  File "/usr/lib/python3.8/subprocess.py", line 364, in check_call
    raise CalledProcessError(retcode, cmd)
subprocess.CalledProcessError: Command '['/usr/bin/gcc', '/tmp/tmpq6ypw6hj/main.c', '-O3', '-I/usr/local/cuda/include', '-I/usr/include/python3.8', '-I/tmp/tmpq6ypw6hj', '-shared', '-fPIC', '-lcuda', '-o', '/tmp/tmpq6ypw6hj/triton_.cpython-38-x86_64-linux-gnu.so']' returned non-zero exit status 1.
"""
The above exception was the direct cause of the following exception:
Traceback (most recent call last):
  File "/home/lume/.local/lib/python3.8/site-packages/torch/_dynamo/output_graph.py", line 680, in call_user_compiler
    compiled_fn = compiler_fn(gm, self.fake_example_inputs())
  File "/home/lume/.local/lib/python3.8/site-packages/torch/_dynamo/debug_utils.py", line 1032, in debug_wrapper
    compiled_gm = compiler_fn(gm, example_inputs, **kwargs)
  File "/home/lume/.local/lib/python3.8/site-packages/torch/__init__.py", line 1234, in __call__
    return self.compile_fn(model_, inputs_)
  File "/home/lume/.local/lib/python3.8/site-packages/torch/_inductor/compile_fx.py", line 398, in compile_fx
    return aot_autograd(
  File "/home/lume/.local/lib/python3.8/site-packages/torch/_dynamo/optimizations/training.py", line 78, in compiler_fn
    cg = aot_module_simplified(gm, example_inputs, **kwargs)
  File "/home/lume/.local/lib/python3.8/site-packages/torch/_functorch/aot_autograd.py", line 2355, in aot_module_simplified
    compiled_fn = create_aot_dispatcher_function(
  File "/home/lume/.local/lib/python3.8/site-packages/torch/_dynamo/utils.py", line 88, in time_wrapper
    r = func(*args, **kwargs)
  File "/home/lume/.local/lib/python3.8/site-packages/torch/_functorch/aot_autograd.py", line 2052, in create_aot_dispatcher_function
    compiled_fn = compiler_fn(flat_fn, fake_flat_tensor_args, aot_config)
  File "/home/lume/.local/lib/python3.8/site-packages/torch/_functorch/aot_autograd.py", line 1307, in aot_wrapper_dedupe
    return compiler_fn(flat_fn, leaf_flat_args, aot_config)
  File "/home/lume/.local/lib/python3.8/site-packages/torch/_functorch/aot_autograd.py", line 957, in aot_dispatch_base
    compiled_fw = aot_config.fw_compiler(fw_module, flat_args)
  File "/home/lume/.local/lib/python3.8/site-packages/torch/_dynamo/utils.py", line 88, in time_wrapper
    r = func(*args, **kwargs)
  File "/home/lume/.local/lib/python3.8/site-packages/torch/_inductor/compile_fx.py", line 373, in fw_compiler
    return inner_compile(
  File "/home/lume/.local/lib/python3.8/site-packages/torch/_dynamo/debug_utils.py", line 588, in debug_wrapper
    compiled_fn = compiler_fn(gm, example_inputs, **kwargs)
  File "/home/lume/.local/lib/python3.8/site-packages/torch/_inductor/debug.py", line 223, in inner
    return fn(*args, **kwargs)
  File "/usr/lib/python3.8/contextlib.py", line 75, in inner
    return func(*args, **kwds)
  File "/home/lume/.local/lib/python3.8/site-packages/torch/_inductor/compile_fx.py", line 140, in compile_fx_inner
    compiled_fn = graph.compile_to_fn()
  File "/home/lume/.local/lib/python3.8/site-packages/torch/_inductor/graph.py", line 538, in compile_to_fn
    return self.compile_to_module().call
  File "/home/lume/.local/lib/python3.8/site-packages/torch/_dynamo/utils.py", line 88, in time_wrapper
    r = func(*args, **kwargs)
  File "/home/lume/.local/lib/python3.8/site-packages/torch/_inductor/graph.py", line 527, in compile_to_module
    mod = PyCodeCache.load(code)
  File "/home/lume/.local/lib/python3.8/site-packages/torch/_inductor/codecache.py", line 468, in load
    exec(code, mod.__dict__, mod.__dict__)
  File "/tmp/torchinductor_lume/gi/cgib6qo3eq2ezv2y7jk7fbgsgl6su24mtz6w5sufxf6tpvhqmcy4.py", line 981, in <module>
    async_compile.wait(globals())
  File "/home/lume/.local/lib/python3.8/site-packages/torch/_inductor/codecache.py", line 663, in wait
    scope[key] = result.result()
  File "/home/lume/.local/lib/python3.8/site-packages/torch/_inductor/codecache.py", line 521, in result
    self.future.result()
  File "/usr/lib/python3.8/concurrent/futures/_base.py", line 444, in result
    return self.__get_result()
  File "/usr/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
subprocess.CalledProcessError: Command '['/usr/bin/gcc', '/tmp/tmpq6ypw6hj/main.c', '-O3', '-I/usr/local/cuda/include', '-I/usr/include/python3.8', '-I/tmp/tmpq6ypw6hj', '-shared', '-fPIC', '-lcuda', '-o', '/tmp/tmpq6ypw6hj/triton_.cpython-38-x86_64-linux-gnu.so']' returned non-zero exit status 1.
The above exception was the direct cause of the following exception:
Traceback (most recent call last):
  File "train.py", line 245, in <module>
    losses = estimate_loss()
  File "/home/lume/.local/lib/python3.8/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
  File "train.py", line 205, in estimate_loss
    logits, loss = model(X, Y)
  File "/home/lume/.local/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1482, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/lume/.local/lib/python3.8/site-packages/torch/_dynamo/eval_frame.py", line 83, in forward
    return self.dynamo_ctx(self._orig_mod.forward)(*args, **kwargs)
  File "/home/lume/.local/lib/python3.8/site-packages/torch/_dynamo/eval_frame.py", line 212, in _fn
    return fn(*args, **kwargs)
  File "/home/lume/.local/lib/python3.8/site-packages/torch/_dynamo/eval_frame.py", line 333, in catch_errors
    return callback(frame, cache_size, hooks)
  File "/home/lume/.local/lib/python3.8/site-packages/torch/_dynamo/convert_frame.py", line 480, in _convert_frame
    result = inner_convert(frame, cache_size, hooks)
  File "/home/lume/.local/lib/python3.8/site-packages/torch/_dynamo/convert_frame.py", line 103, in _fn
    return fn(*args, **kwargs)
  File "/home/lume/.local/lib/python3.8/site-packages/torch/_dynamo/utils.py", line 88, in time_wrapper
    r = func(*args, **kwargs)
  File "/home/lume/.local/lib/python3.8/site-packages/torch/_dynamo/convert_frame.py", line 339, in _convert_frame_assert
    return _compile(
  File "/home/lume/.local/lib/python3.8/site-packages/torch/_dynamo/convert_frame.py", line 400, in _compile
    out_code = transform_code_object(code, transform)
  File "/home/lume/.local/lib/python3.8/site-packages/torch/_dynamo/bytecode_transformation.py", line 341, in transform_code_object
    transformations(instructions, code_options)
  File "/home/lume/.local/lib/python3.8/site-packages/torch/_dynamo/convert_frame.py", line 387, in transform
    tracer.run()
  File "/home/lume/.local/lib/python3.8/site-packages/torch/_dynamo/symbolic_convert.py", line 1684, in run
    super().run()
  File "/home/lume/.local/lib/python3.8/site-packages/torch/_dynamo/symbolic_convert.py", line 538, in run
    and self.step()
  File "/home/lume/.local/lib/python3.8/site-packages/torch/_dynamo/symbolic_convert.py", line 501, in step
    getattr(self, inst.opname)(inst)
  File "/home/lume/.local/lib/python3.8/site-packages/torch/_dynamo/symbolic_convert.py", line 1750, in RETURN_VALUE
    self.output.compile_subgraph(self)
  File "/home/lume/.local/lib/python3.8/site-packages/torch/_dynamo/output_graph.py", line 557, in compile_subgraph
    self.compile_and_call_fx_graph(tx, pass2.graph_output_vars(), root)
  File "/home/lume/.local/lib/python3.8/site-packages/torch/_dynamo/output_graph.py", line 604, in compile_and_call_fx_graph
    compiled_fn = self.call_user_compiler(gm)
  File "/home/lume/.local/lib/python3.8/site-packages/torch/_dynamo/output_graph.py", line 685, in call_user_compiler
    raise BackendCompilerFailed(self.compiler_fn, e) from e
torch._dynamo.exc.BackendCompilerFailed: debug_wrapper raised CalledProcessError: Command '['/usr/bin/gcc', '/tmp/tmpq6ypw6hj/main.c', '-O3', '-I/usr/local/cuda/include', '-I/usr/include/python3.8', '-I/tmp/tmpq6ypw6hj', '-shared', '-fPIC', '-lcuda', '-o', '/tmp/tmpq6ypw6hj/triton_.cpython-38-x86_64-linux-gnu.so']' returned non-zero exit status 1.
Set torch._dynamo.config.verbose=True for more information
You can suppress this exception and fall back to eager by setting:
    torch._dynamo.config.suppress_errors = True